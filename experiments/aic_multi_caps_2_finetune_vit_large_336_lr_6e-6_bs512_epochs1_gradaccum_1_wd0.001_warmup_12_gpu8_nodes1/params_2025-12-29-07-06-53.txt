accum_freq: 1
aggregate: True
batch_size: 512
bert_weight_path: None
beta1: 0.9
beta2: 0.98
checkpoint_path: /home/lizongshu/projects/Chinese-CLIP/experiments/aic_multi_caps_2_finetune_vit_large_336_lr_6e-6_bs512_epochs1_gradaccum_1_wd0.001_warmup_12_gpu8_nodes1/checkpoints
clip_weight_path: None
context_length: 512
debug: False
device: cuda:0
distillation: False
eps: 1e-06
freeze_vision: False
gather_with_grad: False
grad_checkpointing: True
kd_loss_weight: 0.5
local_device_rank: 0
log_interval: 1
log_level: 20
log_path: /home/lizongshu/projects/Chinese-CLIP/experiments/aic_multi_caps_2_finetune_vit_large_336_lr_6e-6_bs512_epochs1_gradaccum_1_wd0.001_warmup_12_gpu8_nodes1/out_2025-12-29-07-06-53.log
logs: /home/lizongshu/projects/Chinese-CLIP/experiments/
lr: 6e-06
mask_ratio: 0
max_epochs: 1
max_steps: 1242
mlm_loss_weight: 1.0
name: aic_multi_caps_2_finetune_vit_large_336_lr_6e-6_bs512_epochs1_gradaccum_1_wd0.001_warmup_12_gpu8_nodes1
num_workers: 4
precision: amp
rank: 0
report_training_batch_acc: True
reset_data_offset: True
reset_optimizer: True
resume: /home/lizongshu/projects/Chinese-CLIP/experiments/concat_wukong_zero_aic_and_aic_and_coco_finetune_vit_large_336_lr_9e-6_bs512_epochs1_gradaccum_4_wd0.001_warmup_10_gpu8_nodes1/checkpoints/epoch1.pt
save_epoch_frequency: 1
save_step_frequency: 999999
seed: 123
skip_aggregate: False
skip_scheduler: False
teacher_model_name: None
text_mask_ratio: 0.0
text_model: RoBERTa-wwm-ext-base-chinese
train_data: /home/lizongshu/projects/Chinese-CLIP/datasets/aic-filter-105w-zh-multi-caps-2/lmdb/train
use_augment: True
use_bn_sync: False
use_flash_attention: False
val_data: /home/lizongshu/projects/Chinese-CLIP/datasets/aic-filter-105w-zh-multi-caps-2/lmdb/test
valid_batch_size: 128
valid_epoch_interval: 1
valid_num_workers: 1
valid_step_interval: 2000
vision_model: ViT-L-14-336
vtm_hard_sample_num: None
vtm_hard_sample_ratio: None
vtm_loss: False
vtm_loss_weight: 1.0
warmup: 12
wd: 0.001
world_size: 8
